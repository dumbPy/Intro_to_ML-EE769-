{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hiino7HiyD8w"
   },
   "source": [
    "\n",
    "# Google Landmark Recognition Challenge\n",
    "\n",
    "### by-\n",
    "   ### Sufiyan Adhikari (173190009)\n",
    "   ### Jaswant Singh    (173190020)\n",
    "   ### Khyati Thakkar   (173194001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4156,
     "status": "ok",
     "timestamp": 1525074838486,
     "user": {
      "displayName": "Sufiyan Adhikari",
      "photoUrl": "//lh3.googleusercontent.com/-5v7VMxmmv1Q/AAAAAAAAAAI/AAAAAAAAAWE/402P87Q3Mm8/s50-c-k-no/photo.jpg",
      "userId": "109884950391033451317"
     },
     "user_tz": -330
    },
    "id": "lZ2hxBSimgm0",
    "outputId": "c84bb975-a15f-401c-b178-e6fd476bd7a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>landmark_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cacf8152e2d2ae60</td>\n",
       "      <td>http://static.panoramio.com/photos/original/70...</td>\n",
       "      <td>4676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0a58358a2afd3e4e</td>\n",
       "      <td>http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...</td>\n",
       "      <td>6651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6b2bb500b6a38aa0</td>\n",
       "      <td>http://lh6.ggpht.com/-vKr5G5MEusk/SR6r6SJi6mI/...</td>\n",
       "      <td>11284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b399f09dee9c3c67</td>\n",
       "      <td>https://lh3.googleusercontent.com/-LOW2cjAqubA...</td>\n",
       "      <td>8429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19ace29d77a5be66</td>\n",
       "      <td>https://lh5.googleusercontent.com/-tnmSXwQcWL8...</td>\n",
       "      <td>6231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url  \\\n",
       "0  cacf8152e2d2ae60  http://static.panoramio.com/photos/original/70...   \n",
       "1  0a58358a2afd3e4e  http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...   \n",
       "2  6b2bb500b6a38aa0  http://lh6.ggpht.com/-vKr5G5MEusk/SR6r6SJi6mI/...   \n",
       "3  b399f09dee9c3c67  https://lh3.googleusercontent.com/-LOW2cjAqubA...   \n",
       "4  19ace29d77a5be66  https://lh5.googleusercontent.com/-tnmSXwQcWL8...   \n",
       "\n",
       "   landmark_id  \n",
       "0         4676  \n",
       "1         6651  \n",
       "2        11284  \n",
       "3         8429  \n",
       "4         6231  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vIpvUwBrspFG"
   },
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Y3NtqpjnmXa"
   },
   "source": [
    "### Data Sampling\n",
    "\n",
    "As Data size is Huge and Data is [Highly Skewed](https://www.kaggle.com/codename007/a-very-extensive-landmark-exploratory-analysis),  \n",
    "Sampling was required to make sure everything works before training on whole data.\n",
    "\n",
    "train: 336 GB with 1,220,165 images \n",
    "test: 34.9 GB with 116,163 images\n",
    "\n",
    "Data was downloaded [with]() this script, reducing the resolution to *299* from *1600*\n",
    "as demonstrated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1177,
     "status": "ok",
     "timestamp": 1525077981271,
     "user": {
      "displayName": "Sufiyan Adhikari",
      "photoUrl": "//lh3.googleusercontent.com/-5v7VMxmmv1Q/AAAAAAAAAAI/AAAAAAAAAWE/402P87Q3Mm8/s50-c-k-no/photo.jpg",
      "userId": "109884950391033451317"
     },
     "user_tz": -330
    },
    "id": "iYaHFEzCrqaK",
    "outputId": "a7ba3274-5401-4656-b677-f87844aaf0a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmark IDs with Very Few Photos: \n",
      "\n",
      "   Photos  Id Count\n",
      "0       1       164\n",
      "1       2       303\n",
      "2       3       632\n",
      "3       4      1022\n",
      "4       5      1308\n",
      "\n",
      "\n",
      "Landmarks IDs with Very High Number of Photos \n",
      "\n",
      "     Photos  Id Count\n",
      "774   13208         1\n",
      "775   18328         1\n",
      "776   23261         1\n",
      "777   49880         1\n",
      "778   50079         1\n"
     ]
    }
   ],
   "source": [
    "def show_count(train):\n",
    "    temp = pd.DataFrame(train.landmark_id.value_counts())\n",
    "    temp.reset_index(inplace=True)\n",
    "    temp.columns = ['landmark_id','count1']\n",
    "    temp = pd.DataFrame(temp.count1.value_counts())\n",
    "    temp = temp.sort_index()\n",
    "    temp.reset_index(inplace=True)\n",
    "    temp.columns = [\"Photos\",\"Id Count\"]\n",
    "    print(\"Landmark IDs with Very Few Photos: \\n\")\n",
    "    print(temp.head())\n",
    "    print(\"\\n\\nLandmarks IDs with Very High Number of Photos \\n\")\n",
    "    print(temp.tail())\n",
    "show_count(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\t\t       tmp\t       train_30_4.csv  train.csv\r\n",
      "planet\t\t       train\t       train_30_5.csv  train_val_30_10.pkl\r\n",
      "sample_submission.csv  train_30_0.csv  train_30_6.csv  train_val_30_5.pkl\r\n",
      "temp_1\t\t       train_30_1.csv  train_30_7.csv\r\n",
      "test\t\t       train_30_2.csv  train_30_8.csv\r\n",
      "test.csv\t       train_30_3.csv  train_30.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Td0TFnOxr88B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getenv(\"HOME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rhaZXPo3nIic"
   },
   "source": [
    "### Reducing Train Data and Extracting Validation Data\n",
    "As we can see above, 159 Landmark IDs have only 1 photo, 259 have only 2 photos, etc.\n",
    "where as, a few IDs have photos in excess of several thousand.\n",
    "The Below function was written to take in the original train.csv file and reduce it to include maximum *trn_sz* photos of each landmark for training, and also generate maximum *val_sz* of validation photo indexes of each landmark if it has more than *trn*sz* photos and save it in a python list as required by **fastai** library\n",
    "\n",
    "It Dumps these into a pickle file with name train_val_<trn_sz>_<val_sz>.pkl\n",
    "\n",
    "and can be imported as *train, val_idxs = joblib.load(filename)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Photos Not Downloaded:  0\n"
     ]
    }
   ],
   "source": [
    "missing_in_train = []\n",
    "for i, row in train.iterrows():\n",
    "    filename = f'{HOME}/data/train/{row[\"id\"]}.jpg'\n",
    "    if not os.path.exists(filename): \n",
    "        missing_in_train.append(i)\n",
    "    elif 0 == os.path.getsize(filename):\n",
    "        #Deleting the empty files\n",
    "        os.unlink(filename)\n",
    "        missing_in_train.append(i)\n",
    "print(\"Number of Photos Not Downloaded: \", len(missing_in_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generate_train_val_set(trn_sz=None, val_sz=10):\n",
    "    if trn_sz==None:\n",
    "        print(\"Train Size Not given. Exiting Train Set Generation!!!\")        \n",
    "        return()        \n",
    "\n",
    "    import os\n",
    "    import numpy as np \n",
    "    import pandas as pd\n",
    "    from sklearn.externals import joblib #used for pickle file handeling    \n",
    "    cur_dir=os.getcwd()\n",
    "    os.chdir(f'{HOME}/data')    \n",
    "    filename=f'train_val_{trn_sz}_{val_sz}.pkl'\n",
    "    if os.path.exists(filename):\n",
    "        print(f'Train and Validation Data with trn_sz={trn_sz}, val_sz={val_sz} already exists:')\n",
    "        return()\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    print(\"Actual Training Data Size: \",train_data.shape)\n",
    "    temp=pd.DataFrame(train_data.landmark_id.value_counts())\n",
    "    temp.reset_index(inplace=True)\n",
    "    temp.columns = ['landmark_id','count']\n",
    "\n",
    "    \n",
    "    new_train=pd.DataFrame()\n",
    "    #Validation Set is only defined as indexes of train.csv\n",
    "    new_val_idxs=[]\n",
    "    for i in range(len(train_data['landmark_id'].unique())):\n",
    "        a=temp['landmark_id'][i]\n",
    "        b=train_data[train_data['landmark_id']==a]\n",
    "        val_flag = 0\n",
    "        if(temp['count'][i]>trn_sz):\n",
    "            val_flag = 1\n",
    "            t=b.iloc[0:trn_sz,[0, 2]]\n",
    "            if(temp['count'][i]<=trn_sz+val_sz):\n",
    "                v_sz=temp['count'][i]-trn_sz\n",
    "            else:\n",
    "                v_sz=val_sz\n",
    "            new_train=pd.concat([new_train,t],axis=0, ignore_index=True)\n",
    "            new_val_idxs=new_val_idxs+new_train.index.values.tolist()[-1:-1-v_sz:-1]\n",
    "        else:\n",
    "            new_train=pd.concat([new_train,b],axis=0)\n",
    "    new_train.drop('url', inplace=True, axis=1)\n",
    "    new_train.to_csv(f'train_{trn_sz}.csv', index=False)   \n",
    "    filename=f'train_val_{trn_sz}_{val_sz}.pkl'\n",
    "    joblib.dump((new_train, new_val_idxs), filename)\n",
    "    print(f'Train Dataframe and Val_idxs dumped to {filename}')\n",
    "    print(f'Train Data also saved to train_{trn_sz}.csv')\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_val_set(trn_sz=None, val_sz=5):\n",
    "    if trn_sz==None:\n",
    "        print(\"Train Size Not given. Exiting Train Set Generation!!!\")        \n",
    "        return()        \n",
    "\n",
    "    import os\n",
    "    import numpy as np \n",
    "    import pandas as pd\n",
    "    from sklearn.externals import joblib #used for pickle file handeling    \n",
    "    cur_dir=os.getcwd()\n",
    "    os.chdir(f'{HOME}/data')    \n",
    "    filename=f'train_val_{trn_sz}_{val_sz}.pkl'\n",
    "    if os.path.exists(filename):\n",
    "        print(f'Train and Validation Data with trn_sz={trn_sz}, val_sz={val_sz} already exists:')\n",
    "        return()\n",
    "    train = pd.read_csv('train.csv')\n",
    "    print(\"Actual Training Data Size: \",train_data.shape)\n",
    "    max_count=dict(train_data.landmark_id.value_counts())\n",
    "    keys = max_count.keys()\n",
    "    \n",
    "    new_train=[]\n",
    "    counter_dict = {}\n",
    "    for key in keys: counter_dict[key] = 0\n",
    "    #Validation Set is only defined as indexes of train.csv\n",
    "    val_idxs=[]\n",
    "    for i, row in train.iterrows():\n",
    "        key = row['landmark_id']\n",
    "        if counter_dict[key] < trn_sz:\n",
    "            new_train.append(row.values)\n",
    "            counter_dict[key] = counter_dict[key]+1\n",
    "        elif counter_dict[key] < trn_sz+val_sz:\n",
    "            new_train.append(row.values)\n",
    "            counter_dict[key] = counter_dict[key]+1\n",
    "            val_idxs.append(len(new_train)-1)\n",
    "    new_train = pd.DataFrame(new_train, columns = train.columns)\n",
    "    new_train.drop('url', inplace=True, axis=1)\n",
    "    new_train.to_csv(f'train_{trn_sz}.csv', index=False)   \n",
    "    filename=f'train_val_{trn_sz}_{val_sz}.pkl'\n",
    "    joblib.dump((new_train, val_idxs), filename)\n",
    "    print(f'Train Dataframe and Val_idxs dumped to {filename}')\n",
    "    print(f'Train Data also saved to train_{trn_sz}.csv')\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Training Data Size:  (1217714, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "print(\"Actual Training Data Size: \",train_data.shape)\n",
    "temp=pd.DataFrame(train_data.landmark_id.value_counts())\n",
    "temp.reset_index(inplace=True)\n",
    "temp.columns = ['landmark_id','count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 1, 1: 1}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame([[1, 2], [3, 4]], columns=['f', 's'])\n",
    "dict(a.f.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Training Data Size:  (1217714, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landmark_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9633</td>\n",
       "      <td>50079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6051</td>\n",
       "      <td>49880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6599</td>\n",
       "      <td>23261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9779</td>\n",
       "      <td>18328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2061</td>\n",
       "      <td>13208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   landmark_id  count\n",
       "0         9633  50079\n",
       "1         6051  49880\n",
       "2         6599  23261\n",
       "3         9779  18328\n",
       "4         2061  13208"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "print(\"Actual Training Data Size: \",train_data.shape)\n",
    "temp=pd.DataFrame(train_data.landmark_id.value_counts())\n",
    "temp.reset_index(inplace=True)\n",
    "temp.columns = ['landmark_id','count']\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>landmark_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cacf8152e2d2ae60</td>\n",
       "      <td>http://static.panoramio.com/photos/original/70...</td>\n",
       "      <td>4676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0a58358a2afd3e4e</td>\n",
       "      <td>http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...</td>\n",
       "      <td>6651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6b2bb500b6a38aa0</td>\n",
       "      <td>http://lh6.ggpht.com/-vKr5G5MEusk/SR6r6SJi6mI/...</td>\n",
       "      <td>11284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b399f09dee9c3c67</td>\n",
       "      <td>https://lh3.googleusercontent.com/-LOW2cjAqubA...</td>\n",
       "      <td>8429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19ace29d77a5be66</td>\n",
       "      <td>https://lh5.googleusercontent.com/-tnmSXwQcWL8...</td>\n",
       "      <td>6231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url  \\\n",
       "0  cacf8152e2d2ae60  http://static.panoramio.com/photos/original/70...   \n",
       "1  0a58358a2afd3e4e  http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...   \n",
       "2  6b2bb500b6a38aa0  http://lh6.ggpht.com/-vKr5G5MEusk/SR6r6SJi6mI/...   \n",
       "3  b399f09dee9c3c67  https://lh3.googleusercontent.com/-LOW2cjAqubA...   \n",
       "4  19ace29d77a5be66  https://lh5.googleusercontent.com/-tnmSXwQcWL8...   \n",
       "\n",
       "   landmark_id  \n",
       "0         4676  \n",
       "1         6651  \n",
       "2        11284  \n",
       "3         8429  \n",
       "4         6231  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(HOME)\n",
    "train.drop(index=missing_in_train, inplace=True)\n",
    "train.to_csv(\"data/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sz = 100\n",
    "parts = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Training Data Size:  (1217714, 3)\n",
      "Train Dataframe and Val_idxs dumped to train_val_100_10.pkl\n",
      "Train Data also saved to train_100.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_train_val_set(trn_sz=trn_sz, val_sz=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(HOME)\n",
    "from sklearn.externals import joblib #used for pickle file handeling    \n",
    "train, val_idxs = joblib.load(\"data/train_val_30_10.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(HOME)\n",
    "train = pd.read_csv(f\"data/train_{trn_sz}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>landmark_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cacf8152e2d2ae60</td>\n",
       "      <td>4676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0a58358a2afd3e4e</td>\n",
       "      <td>6651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6b2bb500b6a38aa0</td>\n",
       "      <td>11284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b399f09dee9c3c67</td>\n",
       "      <td>8429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19ace29d77a5be66</td>\n",
       "      <td>6231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  landmark_id\n",
       "0  cacf8152e2d2ae60         4676\n",
       "1  0a58358a2afd3e4e         6651\n",
       "2  6b2bb500b6a38aa0        11284\n",
       "3  b399f09dee9c3c67         8429\n",
       "4  19ace29d77a5be66         6231"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the rows\n",
    "train = train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "temp = train.shape[0]-1\n",
    "sz_ls = [int(i*temp) for i in np.linspace(0, 1, num=parts ,endpoint=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"data/train_{trn_sz}\"):\n",
    "    os.mkdir(f\"data/train_{trn_sz}\")\n",
    "for i in range(len(sz_ls)-1):\n",
    "    train.iloc[sz_ls[i]:sz_ls[i+1], :].to_csv(f'data/train_{trn_sz}/train_{trn_sz}_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To add fastai to system path\n",
    "import sys\n",
    "sys.path.append(f'{HOME}/fastai')\n",
    "\n",
    "\n",
    "\n",
    "# This file contains all the main external libs we'll use\n",
    "from fastai.imports import *\n",
    "\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "\n",
    "PATH = f'{HOME}/data'\n",
    "sz=64\n",
    "f_model = resnet34\n",
    "bs=36\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i in range(3):\n",
    "os.chdir(f'{HOME}/data')\n",
    "i=0\n",
    "label_csv = f'{PATH}/train_{trn_sz}/train_{trn_sz}_{i}.csv'\n",
    "n = len(list(open(label_csv)))-1\n",
    "val_idxs = get_cv_idxs(n)\n",
    "tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\n",
    "data = ImageClassifierData.from_csv(PATH, 'train', label_csv, bs=bs, tfms=tfms,\n",
    "                                    suffix='.jpg', val_idxs=val_idxs, test_name='test')\n",
    "learn = ConvLearner.pretrained(f_model, data, precompute=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lrf=learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(i):\n",
    "    label_csv = f'{PATH}/train_{trn_sz}/train_{trn_sz}_{i}.csv'\n",
    "    n = len(list(open(label_csv)))-1\n",
    "    val_idxs = get_cv_idxs(n)\n",
    "    tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\n",
    "    data = ImageClassifierData.from_csv(PATH, 'train', label_csv, bs=bs, tfms=tfms,\n",
    "                                        suffix='.jpg', val_idxs=val_idxs, test_name='test')\n",
    "    return (data)\n",
    "    \n",
    "learn = ConvLearner.pretrained(f_model, data, precompute=True)\n",
    "for i in range(parts):\n",
    "    learn.set_data(get_data(i))\n",
    "    learn.fit(lr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gX4qAYJLoBs8",
    "XNs7zY9APqfI",
    "H7Y97zTktxOY",
    "SBzhGAububOJ"
   ],
   "default_view": {},
   "name": "Landmark_Recog.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
